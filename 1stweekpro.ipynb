{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d523e67-c37b-4c5c-b3a1-620ddd26de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define the paths for the input and output folders\n",
    "input_folder = \"/home/rubeena-sulthana/Downloads/AMERICAN GOLDFINCH\"\n",
    "output_folder_1 = \"/home/rubeena-sulthana/Downloads/train\"\n",
    "output_folder_2 = \"/home/rubeena-sulthana/Downloads/validation\"\n",
    "output_folder_3 = \"/home/rubeena-sulthana/Downloads/test\"\n",
    "\n",
    "# Create the output folders if they don't exist\n",
    "for folder in [output_folder_1, output_folder_2, output_folder_3]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# Get the list of image files in the input folder\n",
    "image_files = os.listdir(input_folder)\n",
    "\n",
    "# Calculate the number of images for each category\n",
    "total_images = len(image_files)\n",
    "category_1_count = int(total_images * 0.8)\n",
    "category_2_count = int(total_images * 0.1)\n",
    "category_3_count = total_images - category_1_count - category_2_count\n",
    "\n",
    "# Shuffle the image files randomly\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Move the images to the respective folders based on the distribution\n",
    "for i, image_file in enumerate(image_files):\n",
    "    image_path = os.path.join(input_folder, image_file)\n",
    "    if i < category_1_count:\n",
    "        output_path = os.path.join(output_folder_1, image_file)\n",
    "    elif i < category_1_count + category_2_count:\n",
    "        output_path = os.path.join(output_folder_2, image_file)\n",
    "    else:\n",
    "        output_path = os.path.join(output_folder_3, image_file)\n",
    "    shutil.move(image_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9db464d9-7567-48b2-b598-eb0e47243cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Training Accuracy: 100.00%, Avg Loss: 0.01\n",
      "Epoch 2/2, Training Accuracy: 100.00%, Avg Loss: 0.00\n",
      "Validation Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "##1. Implement a perceptron from scratcch \n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths to your training, validation, and testing data\n",
    "output_folder_1 = \"/home/rubeena-sulthana/Downloads/train\"\n",
    "output_folder_2 = \"/home/rubeena-sulthana/Downloads/test\"\n",
    "output_folder_3 = \"/home/rubeena-sulthana/Downloads/validation/\"\n",
    "\n",
    "# Initialize perceptron parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 2\n",
    "input_shape = (28, 28)  # Assuming images are resized to 28x28 and grayscale\n",
    "\n",
    "# Initialize perceptron weights and bias\n",
    "num_inputs = np.prod(input_shape)\n",
    "weights = np.zeros(num_inputs)\n",
    "bias = 0.0\n",
    "\n",
    "# Function to load data from folders\n",
    "def load_data(folder_path, input_shape):\n",
    "    X = []\n",
    "    y = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "                img_path = os.path.join(root, file)\n",
    "                label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "                with Image.open(img_path) as img:\n",
    "                    img = img.convert('L')  # Convert to grayscale\n",
    "                    img = img.resize(input_shape)  # Resize image to match input_shape\n",
    "                    img_data = np.array(img).flatten()\n",
    "                    X.append(img_data)\n",
    "                y.append(label)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# Load training data\n",
    "X_train, y_train = load_data(output_folder_1, input_shape)\n",
    "\n",
    "# Training the perceptron\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X_train)):\n",
    "        linear_output = np.dot(weights, X_train[i]) + bias\n",
    "        prediction = 1 if linear_output >= 0 else 0\n",
    "        error = y_train[i] - prediction\n",
    "        total_loss += error ** 2  # Sum of squared errors (SSE)\n",
    "        weights += learning_rate * error * X_train[i]\n",
    "        bias += learning_rate * error\n",
    "\n",
    "    # Calculate training accuracy and loss after each epoch\n",
    "    correct_train = 0\n",
    "    for i in range(len(X_train)):\n",
    "        linear_output = np.dot(weights, X_train[i]) + bias\n",
    "        prediction = 1 if linear_output >= 0 else 0\n",
    "        if prediction == y_train[i]:\n",
    "            correct_train += 1\n",
    "    training_accuracy = correct_train / len(X_train)\n",
    "    average_loss = total_loss / len(X_train)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {training_accuracy * 100:.2f}%, Avg Loss: {average_loss:.2f}\")\n",
    "\n",
    "# Function to test accuracy on a dataset\n",
    "def test_accuracy(X, y, weights, bias):\n",
    "    correct = 0\n",
    "    for i in range(len(X)):\n",
    "        linear_output = np.dot(weights, X[i]) + bias\n",
    "        prediction = 1 if linear_output >= 0 else 0\n",
    "        if prediction == y[i]:\n",
    "            correct += 1\n",
    "    return correct / len(X)\n",
    "\n",
    "# Load validation data\n",
    "X_val, y_val = load_data(output_folder_2, input_shape)\n",
    "\n",
    "# Validate and print accuracy\n",
    "validation_accuracy = test_accuracy(X_val, y_val, weights, bias)\n",
    "print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Load testing data\n",
    "X_test, y_test = load_data(output_folder_3, input_shape)\n",
    "\n",
    "# Test and print accuracy\n",
    "test_accuracy = test_accuracy(X_test, y_test, weights, bias)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c25b984-3707-4a80-8ee5-7a4b937c277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/train/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/train/MNIST/raw/train-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/train/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/train/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/train/MNIST/raw/train-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/train/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/train/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/train/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/train/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/train/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/train/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/validation/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/validation/MNIST/raw/train-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/validation/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/validation/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/validation/MNIST/raw/train-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/validation/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/validation/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/validation/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/validation/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/validation/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/validation/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/validation/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/test/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/test/MNIST/raw/train-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/test/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/test/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/test/MNIST/raw/train-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/test/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/test/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/test/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/rubeena-sulthana/Downloads/test/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/test/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rubeena-sulthana/Downloads/test/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/rubeena-sulthana/Downloads/test/MNIST/raw\n",
      "\n",
      "Epoch 1, Loss: 0.4324938522745222\n",
      "Epoch 2, Loss: 0.17937505245009952\n",
      "Epoch 3, Loss: 0.13314065074681314\n",
      "Epoch 4, Loss: 0.10452576792573312\n",
      "Epoch 5, Loss: 0.08919755246027064\n",
      "Epoch 6, Loss: 0.07545191423421807\n",
      "Epoch 7, Loss: 0.0662390741503727\n",
      "Epoch 8, Loss: 0.059288980182371595\n",
      "Epoch 9, Loss: 0.05088042819563383\n",
      "Epoch 10, Loss: 0.047420599705091375\n",
      "Training Accuracy: 98.43833333333333 %\n",
      "Test Accuracy: 97.31 %\n",
      "Validation Accuracy: 97.31 %\n"
     ]
    }
   ],
   "source": [
    "##3Create a multi-layer perceptron (MLP) for digit classification (MNIST dataset)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load data from folders\n",
    "train_data = datasets.MNIST('/home/rubeena-sulthana/Downloads/train', download=True, train=True, transform=transform)\n",
    "validation_data = datasets.MNIST('/home/rubeena-sulthana/Downloads/validation', download=True, train=False, transform=transform)\n",
    "test_data = datasets.MNIST('/home/rubeena-sulthana/Downloads/test', download=True, train=False, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Multi-layer perceptron (MLP) model\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(784, 128),  # input layer (28x28) -> hidden layer (128)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),  # hidden layer (128) -> hidden layer (64)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)  # hidden layer (64) -> output layer (10)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/i}')\n",
    "\n",
    "# Evaluate on training set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Training Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Validation Accuracy: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d6318ce-2e50-436e-a207-9cca7e7cbac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rubeena-sulthana/.local/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8653 - loss: 0.3293  \n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.8068e-06 \n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.1535e-08 \n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 9.7900e-10 \n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.3809e-10 \n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.2820e-10 \n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.9033e-11 \n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.6277e-11 \n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.1111e-11 \n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.4875e-11 \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.6015 \n",
      "Training Accuracy: 100.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 1.5547e-10\n",
      "Validation Accuracy: 100.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 5.6456e-13\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "##2Build and train a simple neural network using a framework like TensorFlow or PyTorch\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define paths to your training, validation, and testing data\n",
    "train_dir = \"/home/rubeena-sulthana/Downloads/train\"\n",
    "test_dir = \"/home/rubeena-sulthana/Downloads/test\"\n",
    "val_dir = \"/home/rubeena-sulthana/Downloads/validation\"\n",
    "\n",
    "# Initialize perceptron parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "input_shape = (28, 28, 3)  # Assuming images are resized to 28x28 and have 3 channels\n",
    "\n",
    "# Function to load and preprocess data using TensorFlow\n",
    "def load_data_tf(folder_path, input_shape):\n",
    "    X = []\n",
    "    y = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "                img_path = os.path.join(root, file)\n",
    "                label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=(input_shape[0], input_shape[1]))\n",
    "                img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                X.append(img_array)\n",
    "                y.append(label)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# Load training data\n",
    "X_train, y_train = load_data_tf(train_dir, input_shape)\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train / 255.0\n",
    "\n",
    "# Define the neural network architecture using TensorFlow's Sequential API\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification, so using sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Binary crossentropy for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=num_epochs)\n",
    "\n",
    "# Function to evaluate accuracy on a dataset\n",
    "def evaluate_accuracy(X, y):\n",
    "    X = X / 255.0  # Normalize pixel values to [0, 1]\n",
    "    _, accuracy = model.evaluate(X, y)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate training accuracy\n",
    "train_accuracy = evaluate_accuracy(X_train, y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Load validation data\n",
    "X_val, y_val = load_data_tf(val_dir, input_shape)\n",
    "\n",
    "# Evaluate validation accuracy\n",
    "validation_accuracy = evaluate_accuracy(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Load testing data\n",
    "X_test, y_test = load_data_tf(test_dir, input_shape)\n",
    "\n",
    "# Evaluate test accuracy\n",
    "test_accuracy = evaluate_accuracy(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d7bc8b3-e8ca-4774-ae4c-790f6cb2c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.5211 - loss: 0.9931 - val_accuracy: 1.0000 - val_loss: 0.3260\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.3254 - val_accuracy: 1.0000 - val_loss: 0.3186\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.3172 - val_accuracy: 1.0000 - val_loss: 0.3115\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.3100 - val_accuracy: 1.0000 - val_loss: 0.3041\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.3026 - val_accuracy: 1.0000 - val_loss: 0.2967\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.2952 - val_accuracy: 1.0000 - val_loss: 0.2895\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.2880 - val_accuracy: 1.0000 - val_loss: 0.2824\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.2810 - val_accuracy: 1.0000 - val_loss: 0.2755\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.2741 - val_accuracy: 1.0000 - val_loss: 0.2689\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.2676 - val_accuracy: 1.0000 - val_loss: 0.2625\n",
      "L2 Regularization Train Accuracy: 100.00%\n",
      "L2 Regularization Validation Accuracy: 100.00%\n",
      "L2 Regularization Test Accuracy: 100.00%\n",
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9551 - loss: 0.2289 - val_accuracy: 1.0000 - val_loss: 9.5627e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6768e-05 - val_accuracy: 1.0000 - val_loss: 8.7488e-07\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.0075e-06 - val_accuracy: 1.0000 - val_loss: 3.7038e-08\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.8516e-08 - val_accuracy: 1.0000 - val_loss: 4.0581e-09\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.8116e-09 - val_accuracy: 1.0000 - val_loss: 8.6524e-10\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 5.1071e-09 - val_accuracy: 1.0000 - val_loss: 2.9316e-10\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.0290e-10 - val_accuracy: 1.0000 - val_loss: 1.3762e-10\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.0604e-11 - val_accuracy: 1.0000 - val_loss: 8.1281e-11\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.7182e-10 - val_accuracy: 1.0000 - val_loss: 5.6416e-11\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.6386e-10 - val_accuracy: 1.0000 - val_loss: 4.3838e-11\n",
      "Dropout Train Accuracy: 100.00%\n",
      "Dropout Validation Accuracy: 100.00%\n",
      "Dropout Test Accuracy: 100.00%\n",
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8204 - loss: 0.3524 - val_accuracy: 1.0000 - val_loss: 0.0457\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0493 - val_accuracy: 1.0000 - val_loss: 0.0627\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0656 - val_accuracy: 1.0000 - val_loss: 0.0754\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0774 - val_accuracy: 1.0000 - val_loss: 0.0838\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0850 - val_accuracy: 1.0000 - val_loss: 0.0892\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0899 - val_accuracy: 1.0000 - val_loss: 0.0922\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0925 - val_accuracy: 1.0000 - val_loss: 0.0934\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0935 - val_accuracy: 1.0000 - val_loss: 0.0934\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0933 - val_accuracy: 1.0000 - val_loss: 0.0927\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0924 - val_accuracy: 1.0000 - val_loss: 0.0914\n",
      "Weight Decay Train Accuracy: 100.00%\n",
      "Weight Decay Validation Accuracy: 100.00%\n",
      "Weight Decay Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "##4Experiment with different regularization techniques on a neural network\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assuming OpenCV is installed for image processing\n",
    "train_dir = \"/home/rubeena-sulthana/Downloads/train\"\n",
    "test_dir = \"/home/rubeena-sulthana/Downloads/test\"\n",
    "val_dir = \"/home/rubeena-sulthana/Downloads/validation\"\n",
    "\n",
    "# Initialize parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "input_shape = (28, 28, 3)  # Assuming images are resized to 28x28 and have 3 channels\n",
    "\n",
    "# Regularization parameters\n",
    "l2_regularization = 0.001  # L2 regularization parameter\n",
    "dropout_rate = 0.2  # Dropout rate\n",
    "weight_decay = 0.0001  # Weight decay parameter\n",
    "\n",
    "# Function to load and preprocess data using TensorFlow\n",
    "def load_data_tf(folder_path, input_shape):\n",
    "    X = []\n",
    "    y = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "                img_path = os.path.join(root, file)\n",
    "                label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=(input_shape[0], input_shape[1]))\n",
    "                img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                X.append(img_array)\n",
    "                y.append(label)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# Load training data\n",
    "X_train, y_train = load_data_tf(train_dir, input_shape)\n",
    "X_train = X_train / 255.0\n",
    "\n",
    "# Load testing data\n",
    "X_test, y_test = load_data_tf(test_dir, input_shape)\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Load validation data\n",
    "X_val, y_val = load_data_tf(val_dir, input_shape)\n",
    "X_val = X_val / 255.0\n",
    "\n",
    "# Define the neural network architecture for L2 regularization\n",
    "model_l2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)),\n",
    "    tf.keras.layers.Dropout(dropout_rate),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)),\n",
    "    tf.keras.layers.Dropout(dropout_rate),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(l2_regularization))\n",
    "])\n",
    "\n",
    "# Compile the model for L2 regularization\n",
    "model_l2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with L2 regularization\n",
    "history_l2 = model_l2.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate accuracy for L2 regularization on train data\n",
    "l2_train_accuracy = model_l2.evaluate(X_train, y_train, verbose=0)[1]\n",
    "print(f\"L2 Regularization Train Accuracy: {l2_train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate accuracy for L2 regularization on validation data\n",
    "l2_val_accuracy = model_l2.evaluate(X_val, y_val, verbose=0)[1]\n",
    "print(f\"L2 Regularization Validation Accuracy: {l2_val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate accuracy for L2 regularization on test data\n",
    "l2_test_accuracy = model_l2.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(f\"L2 Regularization Test Accuracy: {l2_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Define the neural network architecture for Dropout regularization\n",
    "model_dropout = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(dropout_rate),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(dropout_rate),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model for Dropout regularization\n",
    "model_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with Dropout regularization\n",
    "history_dropout = model_dropout.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate accuracy for Dropout regularization on train data\n",
    "dropout_train_accuracy = model_dropout.evaluate(X_train, y_train, verbose=0)[1]\n",
    "print(f\"Dropout Train Accuracy: {dropout_train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate accuracy for Dropout regularization on validation data\n",
    "dropout_val_accuracy = model_dropout.evaluate(X_val, y_val, verbose=0)[1]\n",
    "print(f\"Dropout Validation Accuracy: {dropout_val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate accuracy for Dropout regularization on test data\n",
    "dropout_test_accuracy = model_dropout.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(f\"Dropout Test Accuracy: {dropout_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Define the neural network architecture for Weight Decay regularization\n",
    "model_weight_decay = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(weight_decay))\n",
    "])\n",
    "\n",
    "# Compile the model for Weight Decay regularization\n",
    "optimizer_weight_decay = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model_weight_decay.compile(optimizer=optimizer_weight_decay, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with Weight Decay regularization\n",
    "history_weight_decay = model_weight_decay.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate accuracy for Weight Decay regularization on train data\n",
    "weight_decay_train_accuracy = model_weight_decay.evaluate(X_train, y_train, verbose=0)[1]\n",
    "print(f\"Weight Decay Train Accuracy: {weight_decay_train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate accuracy for Weight Decay regularization on validation data\n",
    "weight_decay_val_accuracy = model_weight_decay.evaluate(X_val, y_val, verbose=0)[1]\n",
    "print(f\"Weight Decay Validation Accuracy: {weight_decay_val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate accuracy for Weight Decay regularization on test data\n",
    "weight_decay_test_accuracy = model_weight_decay.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(f\"Weight Decay Test Accuracy: {weight_decay_test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c544d29e-d44d-489b-bdce-2aa7e3f7b708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer...\n",
      "Epoch 1, Dataset: Train, Loss: 2.1376200318336487, Accuracy: 94.73684210526316%\n",
      "Epoch 1, Dataset: Validation, Loss: 4.275240063667297, Accuracy: 85.71428571428571%\n",
      "Epoch 2, Dataset: Train, Loss: 0.34521326422691345, Accuracy: 98.24561403508773%\n",
      "Epoch 2, Dataset: Validation, Loss: 0.6904265284538269, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Train, Loss: 0.11543199978768826, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Validation, Loss: 0.2308639995753765, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Train, Loss: 0.020172011601971462, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Validation, Loss: 0.040344023203942925, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Train, Loss: 0.0024842473483772665, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Validation, Loss: 0.004968494696754533, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Train, Loss: 0.00266523343151448, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Validation, Loss: 0.00533046686302896, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Train, Loss: 0.0019547336269170046, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Validation, Loss: 0.003909467253834009, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Train, Loss: 0.0003216023296772619, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Validation, Loss: 0.0006432046593545238, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Train, Loss: 6.066835339879617e-05, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Validation, Loss: 0.00012133670679759234, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Train, Loss: 0.004581620916724205, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Validation, Loss: 0.00916324183344841, Accuracy: 100.0%\n",
      "Test Accuracy with SGD optimizer: 100.0%\n",
      "Training with Adam optimizer...\n",
      "Epoch 1, Dataset: Train, Loss: 3.070326544118984e-07, Accuracy: 100.0%\n",
      "Epoch 1, Dataset: Validation, Loss: 6.140653088237968e-07, Accuracy: 100.0%\n",
      "Epoch 2, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 2, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Test Accuracy with Adam optimizer: 100.0%\n",
      "Training with RMSprop optimizer...\n",
      "Epoch 1, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 1, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 2, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 2, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Test Accuracy with RMSprop optimizer: 100.0%\n",
      "Training with Adagrad optimizer...\n",
      "Epoch 1, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 1, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 2, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 2, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Test Accuracy with Adagrad optimizer: 100.0%\n",
      "Training with Adadelta optimizer...\n",
      "Epoch 1, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 1, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 2, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 2, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 3, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 4, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 5, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 6, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 7, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 8, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 9, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Train, Loss: 0.0, Accuracy: 100.0%\n",
      "Epoch 10, Dataset: Validation, Loss: 0.0, Accuracy: 100.0%\n",
      "Test Accuracy with Adadelta optimizer: 100.0%\n"
     ]
    }
   ],
   "source": [
    "##5Compare performance with various optimization algorithms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "# Define paths to datasets\n",
    "training_path = \"/home/rubeena-sulthana/Downloads/train\"\n",
    "validation_path = \"/home/rubeena-sulthana/Downloads/validation\"\n",
    "testing_path = \"/home/rubeena-sulthana/Downloads/test\"\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Paths to datasets\n",
    "datasets = [\n",
    "    {'name': 'train', 'path': training_path},\n",
    "    {'name': 'validation', 'path': validation_path},\n",
    "    {'name': 'test', 'path': testing_path}\n",
    "]\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "data_loaders = {}\n",
    "for dataset in datasets:\n",
    "    images = [f for f in os.listdir(dataset['path']) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    data = []\n",
    "    labels = []\n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(dataset['path'], img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = transform(image)\n",
    "        data.append(image)\n",
    "        labels.append(torch.tensor(0))  # Assuming all labels are 0 for simplicity\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    data_tensor = torch.stack(data)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "\n",
    "    # Create TensorDataset and DataLoader\n",
    "    data_loaders[dataset['name']] = DataLoader(TensorDataset(data_tensor, labels_tensor), batch_size=64, shuffle=dataset['name'] == 'train')\n",
    "\n",
    "# Neural network model\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(224 * 224 * 3, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# List of optimization algorithms to compare\n",
    "optimizers = [\n",
    "    {'name': 'SGD', 'optimizer': optim.SGD(net.parameters(), lr=0.01, momentum=0.9)},\n",
    "    {'name': 'Adam', 'optimizer': optim.Adam(net.parameters(), lr=0.001)},\n",
    "    {'name': 'RMSprop', 'optimizer': optim.RMSprop(net.parameters(), lr=0.001)},\n",
    "    {'name': 'Adagrad', 'optimizer': optim.Adagrad(net.parameters(), lr=0.01)},\n",
    "    {'name': 'Adadelta', 'optimizer': optim.Adadelta(net.parameters(), lr=1.0)}\n",
    "]\n",
    "\n",
    "# Training loop\n",
    "for optimizer_info in optimizers:\n",
    "    optimizer_name = optimizer_info['name']\n",
    "    optimizer = optimizer_info['optimizer']\n",
    "    print(f\"Training with {optimizer_name} optimizer...\")\n",
    "\n",
    "    # Train the network\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        net.train()  # Set the model to train mode\n",
    "        for dataset_name in ['train', 'validation']:  # Include validation dataset for validation accuracy\n",
    "            for i, (inputs, labels) in enumerate(data_loaders[dataset_name], 0):\n",
    "                optimizer.zero_grad()\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        for dataset_name in ['train', 'validation']:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in data_loaders[dataset_name]:\n",
    "                    inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                    outputs = net(inputs)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f'Epoch {epoch+1}, Dataset: {dataset_name.capitalize()}, Loss: {running_loss / len(data_loaders[dataset_name])}, Accuracy: {accuracy}%')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loaders['test']:\n",
    "            inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy with {optimizer_name} optimizer: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d54b0-1986-4aa1-85a6-656f8f771f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
